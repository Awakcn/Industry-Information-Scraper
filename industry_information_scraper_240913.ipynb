{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 个doc\n",
      "['3'] 个unique_list\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from time import sleep\n",
    "import chardet\n",
    "\n",
    "from langchain_openai import embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import difflib\n",
    "\n",
    "def generate_keywords(input):\n",
    "    llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", max_tokens=1280, temperature=0.7, top_p=0.9)\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"user\", \n",
    "    '''\n",
    "    请你扮演一名专业的行业分析师，你应该利用你的行业知识和其他相关因素提供专业的分析。\n",
    "\n",
    "    请你给出以下行业以下每个主题各3至5个重要关键词，关键词应包括行业术语、概念或趋势，保证关键词的准确、精炼性和与主题的相关度，逻辑连贯，内容低重复度、有深度。禁止使用\"：\"和\"（）\"，避免使用品牌名称或专用词，关键词要易搜索和理解。\n",
    "    行业：{input}\n",
    "\n",
    "    主题：\n",
    "    1. 行业定义\n",
    "    2. 行业分类\n",
    "    3. 行业特征\n",
    "    4. 发展历程\n",
    "    5. 产业链分析\n",
    "    6. 市场规模\n",
    "    7. 政策分析\n",
    "    8. 竞争格局\n",
    "\n",
    "    使用以下格式：\n",
    "    1. <主题1>\n",
    "    - <关键词1>\n",
    "    - <关键词2>\n",
    "    - <关键词3>\n",
    "    - <关键词4>\n",
    "    - <关键词5>\n",
    "\n",
    "    2. <主题2>\n",
    "    - <关键词1>\n",
    "    ..\n",
    "    ..\n",
    "    8. ..\n",
    "    完成上述任务后，请停止生成任何额外内容。\n",
    "\n",
    "    样例：\n",
    "    1. 行业定义\n",
    "    - 水域资源管理\n",
    "    - 养殖技术进步\n",
    "    - 水产品质量监控\n",
    "    - 海洋生态环境保护\n",
    "    - 农水联动模式\n",
    "\n",
    "    2. 行业分类\n",
    "    - 鱼类养殖\n",
    "    - 虾蟹养殖\n",
    "    - 牛蛙业\n",
    "    - 海参养殖\n",
    "    - 浮游生物养殖\n",
    "\n",
    "    3. 行业特征\n",
    "    - 规模化生产\n",
    "    - 生物多样性\n",
    "    - 投入成本\n",
    "    - 市场周期性\n",
    "    - 环保压力\n",
    "\n",
    "    4. 发展历程\n",
    "    - 人工饲料\n",
    "    - 智能水产养殖\n",
    "    - 全球化市场拓展\n",
    "    - 产业链整合\n",
    "    - 技术驱动革新\n",
    "\n",
    "    5. 产业链分析\n",
    "    - 种苗供应\n",
    "    - 饲料加工\n",
    "    - 养殖基地建设\n",
    "    - 销售和分销\n",
    "\n",
    "    6. 市场规模\n",
    "    - 全球水产养殖产量\n",
    "    - 主要消费国\n",
    "    - 国内市场规模\n",
    "    - 年度报告与预测\n",
    "\n",
    "    7. 政策分析\n",
    "    - 环保政策影响\n",
    "    - 信贷和补贴支持\n",
    "    - 农业补贴调整\n",
    "\n",
    "    8. 竞争格局\n",
    "    - 主要企业竞争态势\n",
    "    - 新进入者威胁\n",
    "    - 品牌差异化策略\n",
    "    - 外资并购与合作\n",
    "    '''\n",
    "    )])\n",
    "\n",
    "    str_output_parser = StrOutputParser()\n",
    "    chain = prompt | llm | str_output_parser\n",
    "\n",
    "    accumulated_text = \"\"\n",
    "    for message in chain.stream({\"input\": input}):\n",
    "        accumulated_text += message\n",
    "        yield accumulated_text\n",
    "#     return '''1. 行业定义\n",
    "# - 水域资源管理  \n",
    "# - 养殖技术进步\n",
    "# - 水产品质量监控\n",
    "# - 海洋生态环境保护\n",
    "# - 农水联动模式\n",
    "# '''\n",
    "\n",
    "def process_keywords(input):\n",
    "    lines = input.strip().split(\"\\n\")\n",
    "    industry_keywords = []\n",
    "    current_topic = \"\"\n",
    "    topics = {}\n",
    "    keywords = []\n",
    "    for line in lines:\n",
    "        if re.match(r\".*?\\d+\\.\\s.*\", line):  # 1. 行业定义\n",
    "            current_topic = line.split(\". \")[1].strip()\n",
    "            topics[current_topic] = []\n",
    "        elif re.match(r\".*?-\\s.*\", line):        # - 行业关键词\n",
    "            keyword = line.split(\"- \")[1].strip()\n",
    "            topics[current_topic].append(keyword)\n",
    "            industry_keywords.append((current_topic, keyword))\n",
    "            keywords.append(keyword)\n",
    "\n",
    "    accumulated_text = \"\"\n",
    "    for topic, keyword in industry_keywords:\n",
    "        accumulated_text += f\"{keyword}\\n\"\n",
    "\n",
    "    return accumulated_text, keywords_to_option(input)\n",
    "\n",
    "def keywords_to_option(input):\n",
    "    lines = input.strip().split(\"\\n\")\n",
    "    industry_keywords = []\n",
    "    current_topic = \"\"\n",
    "    topics = {}\n",
    "    keywords = []\n",
    "    for line in lines:\n",
    "        if re.match(r\".*?\\d+\\.\\s.*\", line):  # 1. 行业定义\n",
    "            current_topic = line.split(\". \")[1].strip()\n",
    "            topics[current_topic] = []\n",
    "        elif re.match(r\".*?-\\s.*\", line):        # - 行业关键词\n",
    "            keyword = line.split(\"- \")[1].strip()\n",
    "            topics[current_topic].append(keyword)\n",
    "            industry_keywords.append((current_topic, keyword))\n",
    "            keywords.append(keyword)\n",
    "\n",
    "    # 打印行业关键词组合\n",
    "    # for topic, keyword in industry_keywords:\n",
    "    #     print(f\"{input} {keyword}\")\n",
    "    # for keyword in keywords:\n",
    "    #     print(f\"{keyword}\")\n",
    "    # accumulated_text = \"\"\n",
    "    # for topic, keyword in industry_keywords:\n",
    "    #     accumulated_text += f\"{keyword}\\n\"\n",
    "\n",
    "    return gr.update(choices=keywords, value=None, interactive=True)\n",
    "\n",
    "\n",
    "def clear_all():\n",
    "    return \"\", \"\", \"\", gr.update(choices=[], value=None, interactive=False)\n",
    "\n",
    "\n",
    "def handle_selection(industry, keywords_to_search, pages_needed):\n",
    "    # 模拟处理函数，可能抛出异常\n",
    "    if industry == \"\" or keywords_to_search == \"\":\n",
    "        return \"请输入行业和关键词\"\n",
    "    ua = UserAgent()\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'accept-encoding': 'gzip, deflate, br, zstd',\n",
    "        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "        'connection': 'keep-alive',\n",
    "        #'cookie': 'PSTM=1710688297; BD_UPN=12314753; BIDUPSID=F67F8916CCCEBCD73956847F5D7978CC; BDUSS=jd6TEVuNlN6akQxVmo1dFpUcjkySEQwYmhqa2dVeXBZWHotQ3RvZGNqRUtWU0ptSVFBQUFBJCQAAAAAAAAAAAEAAABTgqghYWxleDIwMDM3MzEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAArI-mUKyPplV; BDUSS_BFESS=jd6TEVuNlN6akQxVmo1dFpUcjkySEQwYmhqa2dVeXBZWHotQ3RvZGNqRUtWU0ptSVFBQUFBJCQAAAAAAAAAAAEAAABTgqghYWxleDIwMDM3MzEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAArI-mUKyPplV; BAIDUID=FD799B193B0AEF60B5A1A9C9C6EDB6A9:SL=0:NR=10:FG=1; newlogin=1; H_WISE_SIDS_BFESS=60360_60453_60467_60492_60498_60552_60564; H_PS_PSSID=60360_60564; H_WISE_SIDS=60360_60564; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; ab_sr=1.0.1_MDA0Njk3MzdmMWQ4ZjkyZTBlN2E4YjY0NmVjNTAzZWY5YjdmMmIzYjhmMTRhYTAxNjQ3MmE1NmQ4ODg3YTlhNDViODZlNDkyODJhMWE5NWZiZThiMWQxZmUwZGEwNWVlYzM1MzUwMzgwZTdlMTQ5N2FmY2Q0MzFiZjM3NTZjYjQ0Mzc4NTczZjcxNzZkMTIyNWI1OTU5OWVlZGUyODA1YQ==; sug=3; sugstore=0; ORIGIN=0; bdime=0; BA_HECTOR=2lal810k2g0ga42g2501850g845es61jb0q6o1u; delPer=0; BD_CK_SAM=1; PSINO=3; ZFY=juQ5OsocA2UxNHl:BsYgz1ZK8IU76I:AkzN6Zq:A5D2stw:C; BAIDUID_BFESS=FD799B193B0AEF60B5A1A9C9C6EDB6A9:SL=0:NR=10:FG=1; COOKIE_SESSION=242939_0_7_7_4_21_1_2_5_7_1_6_168352_0_0_0_1722497720_0_1722825515%7C9%231532_27_1722233001%7C9; baikeVisitId=bfb245aa-2485-4ca3-bcad-f00e7e4cd8c3; H_PS_645EC=b986QSaUEyz7RSOXAdapMdMhqzQGeF5ck1HMI0tjW4XAHp7jLLVZfnW7E7k; BDSVRTM=173',\n",
    "        'User-Agent': ua.edge\n",
    "    }\n",
    "    all_links = []\n",
    "    keyword_combination = f\"{industry} {keywords_to_search}\"\n",
    "    for pages in range(pages_needed):\n",
    "        page = pages * 10\n",
    "        response = requests.get(f'https://www.baidu.com/s?wd={keyword_combination}&pn={page}', headers=headers)\n",
    "\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        content_left = soup.find('div', attrs={'id': 'content_left'})\n",
    "        all_divs = content_left.find_all('div', attrs={'mu': True})\n",
    "        links = [div.get('mu') for div in all_divs]\n",
    "        all_links.extend(links)\n",
    "    # results = \"\\n\".join(all_links)\n",
    "    results = \"\\n\".join([f\"{i+1}. {link}\" for i, link in enumerate(all_links)])\n",
    "    return results\n",
    "\n",
    "def retry_function(fn, max_retries, delay, industry, keywords_to_search, pages_needed):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = fn(industry, keywords_to_search, pages_needed)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                random_delay = random.uniform(delay-0.5, delay+0.5)\n",
    "                sleep(random_delay)  # 延迟重试\n",
    "            else:\n",
    "                return f\"Error: {str(e)}\"\n",
    "\n",
    "def selection_to_links(industry, keywords_to_search, pages_needed):\n",
    "    return retry_function(handle_selection, max_retries=5, delay=1.5, industry=industry, keywords_to_search=keywords_to_search, pages_needed=pages_needed)\n",
    "\n",
    "\n",
    "# webpage_to_text functions\n",
    "def remove_ads_by_tag(soup):\n",
    "    # 常见的广告类名或ID\n",
    "    tag_keywords = ['next', 'post_top_tie', 'jubao', 'search', 'comment_area', 'share', 'nav', 'ad', 'recommend', 'tool', 'advertisement', 'ads', 'sponsored', 'promo', 'banner', 'adsense', 'aside', 'footer', 'header', 'side-bar', 'column', 'sidebar', 'list', 'sideColumn', 'side']\n",
    "\n",
    "    pattern = re.compile('|'.join(tag_keywords), re.IGNORECASE)\n",
    "\n",
    "    tags = soup.find_all(class_=pattern)\n",
    "    for tag in tags:\n",
    "        tag.decompose()\n",
    "\n",
    "    tags = soup.find_all(id=pattern)\n",
    "    for tag in tags:\n",
    "        tag.decompose()\n",
    "\n",
    "    # 删除头尾侧边栏\n",
    "    for iframe in soup.find_all('iframe'):\n",
    "        iframe.decompose()\n",
    "    for aside in soup.find_all('aside'):\n",
    "        aside.decompose()\n",
    "    for header in soup.find_all('header'):\n",
    "        header.decompose()\n",
    "    for footer in soup.find_all('footer'):\n",
    "        footer.decompose()\n",
    "\n",
    "    return soup\n",
    "\n",
    "def remove_ads_by_text(soup):\n",
    "    ad_keywords = ['优惠券', '阅读原文', '扫一扫', '限时抢购', '免费试用', '立即注册', '超值折扣', '注册有礼', '免费领取', '立即购买', '关注该公众号', '微信扫码', '分享至', '下载(.*?)客户端', '返回(.*?)首页', '阅读下一篇', '特别声明：以上内容', 'Notice: The content above', '打开(.*?)体验更佳', '热搜', '打开(.*?)新闻', '查看精彩图片']\n",
    "    for keyword in ad_keywords:\n",
    "        for ad in soup.find_all(string=re.compile(keyword)):\n",
    "            parent = ad.find_parent()\n",
    "            if parent:\n",
    "                parent.decompose()\n",
    "\n",
    "    return soup\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # 使用BeautifulSoup解析HTML\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    soup = remove_ads_by_tag(soup)\n",
    "    soup = remove_ads_by_text(soup)\n",
    "    \n",
    "    # 去除脚本和样式\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.decompose()\n",
    "\n",
    "    # 插入换行符\n",
    "    for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'tr', 'div', 'br', 'hr']):\n",
    "        tag.insert_after('\\n')\n",
    "\n",
    "    # 提取文本\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # 正则表达式清理多余空格空行\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def webpage_to_text(links_found, min_slider, max_slider, num):\n",
    "    all_links = [line.split('. ', 1)[1] for line in links_found.splitlines()]\n",
    "    accumulated_text = \"\"\n",
    "\n",
    "    minnum = int(min_slider)\n",
    "    maxnum = int(max_slider)\n",
    "\n",
    "    # 使用正则表达式解析 num\n",
    "    num_pattern = r'^(\\d+)(?:-(\\d+))?$'  # 匹配单个数字或范围\n",
    "    match = re.match(num_pattern, num)\n",
    "    if match:\n",
    "        start_num = int(match.group(1))\n",
    "        end_num = int(match.group(2)) if match.group(2) else start_num\n",
    "        minnum = start_num if start_num > 0 else minnum\n",
    "        maxnum = end_num\n",
    "\n",
    "\n",
    "    if maxnum == 0 or maxnum < minnum:\n",
    "        maxnum = minnum\n",
    "\n",
    "    if minnum == 0 or all_links == []:\n",
    "        accumulated_text += \"请选择链接\"\n",
    "        return accumulated_text\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'accept-encoding': 'gzip, deflate, br, zstd',\n",
    "        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "        'connection': 'keep-alive',\n",
    "        'User-Agent': ua.edge\n",
    "    }\n",
    "    count = int(0)\n",
    "    for link in all_links:\n",
    "        count += 1\n",
    "        if count < minnum or count > maxnum:\n",
    "            continue\n",
    "        accumulated_text += f\"第{count}个网页：\\n{link}\\n\"\n",
    "        if re.findall(r'zhihu', link):\n",
    "            accumulated_text += \"Error: Zhihu\\n\\n\"\n",
    "            continue\n",
    "        if re.findall(r'weibo', link):\n",
    "            accumulated_text += \"Error: Sina Visitor System\\n\\n\"\n",
    "            continue\n",
    "        try:\n",
    "            # 获取html的文本内容\n",
    "            response = requests.get(link, headers=headers)\n",
    "\n",
    "            response.raise_for_status()  # 检查请求是否成功\n",
    "\n",
    "            # 检测编码\n",
    "            encoding_result = chardet.detect(response.content)\n",
    "            encoding = encoding_result['encoding']\n",
    "            response.encoding = encoding\n",
    "            if re.findall(r'weixin', link):\n",
    "                response.encoding = 'utf-8'\n",
    "                \n",
    "            # 获取网页标题\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            if soup.title and soup.title.string:\n",
    "                title = soup.title.string\n",
    "                if title == \"百度安全验证\":\n",
    "                    accumulated_text += \"Error: Baidu Security Verification\\n\\n\"\n",
    "                    continue\n",
    "                if re.findall(r'百度文库', title):\n",
    "                    accumulated_text += \"Error: Baidu Wenku\\n\\n\"\n",
    "                    continue\n",
    "                # accumulated_text += f\"{title}\\n\" #不显示标题，会重复\n",
    "\n",
    "            text = clean_html(response.text)\n",
    "            accumulated_text += f\"{text}\\n\\n\"\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            accumulated_text += f\"Failed to retrieve {link}: {e}\\n\\n\"\n",
    "    \n",
    "    return accumulated_text, minnum, maxnum\n",
    "\n",
    "\n",
    "def parse_pages(pages_str, minnum, maxnum):\n",
    "    # 替换分隔符\n",
    "    pages_str = re.sub(r'[;\\/.，。、]', ',', pages_str)\n",
    "    # 拆分字符串\n",
    "    parts = pages_str.replace(' ','').split(',')\n",
    "    \n",
    "    result = []\n",
    "    for part in parts:\n",
    "        # 单个数字\n",
    "        if '-' not in part:\n",
    "            num = int(part)\n",
    "            if minnum <= num <= maxnum:\n",
    "                result.append(num)\n",
    "        else: # 数字范围\n",
    "            start, end = map(int, part.split('-'))\n",
    "            result.extend([num for num in range(start, end + 1) if minnum <= num <= maxnum])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def calculate_final_score(relevance, accuracy, completeness, timeliness, authority, readability):\n",
    "    # 定义每个维度的权重\n",
    "    weights = {\n",
    "        'relevance': 0.25,\n",
    "        'accuracy': 0.20,\n",
    "        'completeness': 0.15,\n",
    "        'timeliness': 0.15,\n",
    "        'authority': 0.15,\n",
    "        'readability': 0.10\n",
    "    }\n",
    "    \n",
    "    # 计算加权总分\n",
    "    final_score = (\n",
    "        relevance * weights['relevance'] +\n",
    "        accuracy * weights['accuracy'] +\n",
    "        completeness * weights['completeness'] +\n",
    "        timeliness * weights['timeliness'] +\n",
    "        authority * weights['authority'] +\n",
    "        readability * weights['readability']\n",
    "    )\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "def calculate_by_vector(text1, text2, min_length=20, split_char=r'[|\\n]'):\n",
    "    # 初始化嵌入模型\n",
    "    embedding = embeddings.OpenAIEmbeddings(check_embedding_ctx_length=False, base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "    # 将文本转换为向量\n",
    "    vector1 = embedding.embed_query(text1)\n",
    "\n",
    "    # 分段\n",
    "    segments = re.split(split_char, text2)\n",
    "\n",
    "    # 设定最小长度限制，过滤掉短句\n",
    "    filtered_segments = [segment.strip() for segment in segments if len(segment.strip()) >= min_length]\n",
    "\n",
    "    # 计算每个段落与主题的相似度\n",
    "    similarities = []\n",
    "    for segment in filtered_segments:\n",
    "        vector2 = embedding.embed_query(segment)\n",
    "        similarity = cosine_similarity([vector1], [vector2])\n",
    "        similarities.append((segment, similarity[0][0]))\n",
    "\n",
    "    # 找到相似度最高的段落\n",
    "    if similarities:\n",
    "        best_segment, best_similarity = max(similarities, key=lambda x: x[1])\n",
    "    else:\n",
    "        best_segment, best_similarity = '', 0.00\n",
    "    return best_similarity, best_segment\n",
    "\n",
    "def batch_analyze_webpage(analyze_all, webpage_text, keywords_processed, webpage_to_analyze, industry, keywords_to_search, minnum, maxnum):\n",
    "    if webpage_to_analyze:\n",
    "        pages_to_analyze = parse_pages(webpage_to_analyze, minnum, maxnum)\n",
    "    if analyze_all == True: # 优先选择全部\n",
    "        pages_to_analyze = range(minnum, maxnum + 1)\n",
    "    \n",
    "    analyzed_webpage = str(\"\")\n",
    "    for i in pages_to_analyze:\n",
    "        analyzed_webpage += f\"正在分析第{i}个网页：\\n\"\n",
    "        yield analyzed_webpage\n",
    "        for part in analyze_webpage(webpage_text, keywords_processed, i, industry, keywords_to_search):\n",
    "            yield analyzed_webpage + part\n",
    "        # analyzed_webpage += analyze_webpage(webpage_text, keywords_processed, i, industry, keywords_to_search)\n",
    "        analyzed_webpage += part\n",
    "\n",
    "def analyze_webpage(webpage_text, keywords_processed, webpage_to_analyze, industry, keywords_to_search):\n",
    "    accumulated_text = \"\"\n",
    "    llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", max_tokens=512, temperature=0.6, top_p=0.8, stop_sequences=[\"---END---\"])\n",
    "    n = webpage_to_analyze  # 要提取的页面编号\n",
    "    weblink = re.search(rf'第{n}个网页：\\n(https?:\\/\\/[^\\s]+)', webpage_text)\n",
    "    pattern = rf'第{n}个网页：\\n(?:https?:\\/\\/[^\\n]*\\n)?([\\s\\S]*?)(?=\\n第\\d+个网页：|\\Z)'\n",
    "    match = re.search(pattern, webpage_text)\n",
    "    if match:\n",
    "        content = match.group(1).strip()  # 提取并去除首尾空格\n",
    "    \n",
    "        prompt0 = ChatPromptTemplate.from_messages([(\"user\", \n",
    "'''\n",
    "请判断以下网页爬虫获取的、与{input}行业以及{keywords_to_search}相关的文章页面是否存在缺失（只有标题、没有正文、字数过少、少于100字等）或报错（Error、Failed、错误、验证码等）。请忽略页面中的可能出现的广告、推荐、导航栏等无关信息。\n",
    "文章：\n",
    "{text}\n",
    "\n",
    "若存在缺失或报错，则直接输出\"Error---END---\"；文章完整，则直接输出\"Complete---END---\"。\n",
    "''')])\n",
    "        chain0 = prompt0 | llm | StrOutputParser()\n",
    "        reply0 = chain0.invoke({\"input\": industry, \"text\": content, \"keywords_to_search\": keywords_to_search})\n",
    "\n",
    "        if re.search(r'Error', reply0, re.IGNORECASE):\n",
    "            reply0 = chain0.invoke({\"input\": industry, \"text\": content, \"keywords_to_search\": keywords_to_search})\n",
    "            if re.search(r'Error', reply0, re.IGNORECASE):\n",
    "                iscomplete = False\n",
    "            else:\n",
    "                iscomplete = True\n",
    "            iscomplete = False\n",
    "        else:\n",
    "            iscomplete = True\n",
    "        if not iscomplete:\n",
    "            accumulated_text += f\"网址：{weblink[1]}\\n\\n文章内容缺失。\\n\\n综合评分：0/100\\n向量评分：0/100（最高余弦相似度）\\n----------\\n\\n\"\n",
    "            # return \"文章内容缺失。\\n相关性评分：0/100\\n----------\\n\\n\" #, gr.update(choices=[\"Error\"], value=None, interactive=False)\n",
    "            yield accumulated_text\n",
    "            return\n",
    "\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([(\"user\", \n",
    "'''\n",
    "#START#-#END#为输出格式的范围，[]内为需要填写的部分。\n",
    "\n",
    "第一步：请阅读以下与{input}行业及{keywords_to_search}的文章，并根据文章内容写一个自然段的简介，字数不超过300字。请忽略文章中的广告、推荐和导航栏等无关信息。\n",
    "\n",
    "文章：\n",
    "{text}\n",
    "\n",
    "输出格式：#START#\n",
    "标题：[此处填写标题]\n",
    "简介：\n",
    "[此处撰写简介（字数不超过300字）]\n",
    "#END#\n",
    "\n",
    "第二步：根据文章内容，提取与{input}行业以及{keywords_to_search}相关的**至多8个**关键词。\n",
    "\n",
    "可参考的关键词：（\n",
    "{keywords}）\n",
    "\n",
    "输出格式：#START#\n",
    "关键词：[关键词1]；[关键词2]；...；[关键词n（关键词最多8个）]\n",
    "#END#\n",
    "\n",
    "第三步：请对文章与{input}行业及{keywords_to_search}的相关性等各维度进行评分，范围是0到100，现在是2024年。\n",
    "\n",
    "评分标准：尽量不给90-100，优秀文章给70-89，普通文章给50-69，质量较差给30-49\n",
    "\n",
    "请按以下格式输出：#START#\n",
    "相关性：[n]/100\n",
    "准确性：[n]/100\n",
    "完整性：[n]/100\n",
    "时效性：[n]/100\n",
    "权威性：[n]/100\n",
    "可读性：[n]/100\n",
    "---END---\n",
    "#END#\n",
    "---END---\n",
    "''')])\n",
    "        prompt2 = ChatPromptTemplate.from_messages([(\"user\", \n",
    "'''\n",
    "严格按照以下格式输出：\n",
    "标题：\n",
    "简介：（字数不超过300字）\n",
    "关键词：可选关键词a；可选关键词x；...；可选关键词n\n",
    "评分：n/100\n",
    "\n",
    "完成上述任务后，直接停止生成，不要理由、分析、综述。\n",
    "{prev}\n",
    "''')])\n",
    "        \n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        # reply = chain.invoke({\"input\": industry, \"text\": content, \"keywords\": keywords_processed, \"keywords_to_search\": keywords_to_search})\n",
    "        chain2 = {\"prev\": chain} | prompt2 | llm | StrOutputParser() # 停用\n",
    "        # reply = chain2.invoke({\"input\": industry, \"text\": content, \"keywords\": keywords_processed, \"keywords_to_search\": keywords_to_search})\n",
    "        # reply = f\"网址：{weblink[1]}\\n\\n\" + reply + \"\\n----------\\n\\n\"\n",
    "\n",
    "        accumulated_text += f\"网址：{weblink[1]}\\n\"\n",
    "        yield accumulated_text\n",
    "        for message in chain.stream({\"input\": industry, \"text\": content, \"keywords\": keywords_processed, \"keywords_to_search\": keywords_to_search}):\n",
    "            accumulated_text += message\n",
    "            accumulated_text = accumulated_text.replace(\"---END---\", \"\").replace(\"#START#\", \"\").replace(\"#END#\", \"\")\n",
    "            yield accumulated_text\n",
    "\n",
    "        # 匹配分数\n",
    "        pattern = r\"(?P<dimension>[\\u4e00-\\u9fa5]+)：(?P<score>\\d+)/100\"\n",
    "        matches = re.findall(pattern, accumulated_text)\n",
    "        scores = {dimension: int(score) for dimension, score in matches}\n",
    "        if scores.get(\"综合评分\", 60) == 0:\n",
    "            final_score = 0\n",
    "        else:\n",
    "            final_score = calculate_final_score(scores.get(\"相关性\", 0), scores.get(\"准确性\", 0), scores.get(\"完整性\", 0), scores.get(\"时效性\", 0), scores.get(\"权威性\", 0), scores.get(\"可读性\", 0))\n",
    "            accumulated_text += f\"综合评分：{round(final_score, 2)}/100\\n\"\n",
    "\n",
    "            cosine_similarity = calculate_by_vector(industry + ' ' + keywords_to_search, content, 20, r'[|\\n]')[0]\n",
    "            accumulated_text += f\"向量评分：{cosine_similarity*100:.0f}/100（最高余弦相似度）\\n\"\n",
    "            accumulated_text += \"----------\\n\\n\"\n",
    "        accumulated_text = re.sub(r'\\n{3,}', '\\n\\n', accumulated_text)\n",
    "        yield accumulated_text\n",
    "        # return reply #, match_keywords(reply)\n",
    "    else:\n",
    "        accumulated_text += f\"未找到第{n}个页面的内容\"\n",
    "        yield accumulated_text #, gr.update(choices=[\"Error\"], value=None, interactive=False)\n",
    "\n",
    "def match_keywords(text):\n",
    "    # 正则表达式只匹配“关键词：”这一行后面的内容\n",
    "    pattern = r'^关键词：([^\\n]+)'\n",
    "\n",
    "    # 查找匹配项\n",
    "    match = re.search(pattern, text, re.MULTILINE)\n",
    "\n",
    "    if match:\n",
    "        keywords = match.group(1).strip().replace('.', '').replace('。', '')  # 提取并去除首尾空格\n",
    "        # print(keywords)\n",
    "        # keyword_list = keywords.split('，')  # 将关键词按逗号分隔并存入数组\n",
    "        # if not keyword_list:\n",
    "        #     keyword_list = keywords.split(', ')\n",
    "        # if not keyword_list:\n",
    "        #     keyword_list = keywords.split(',')\n",
    "        # if not keyword_list:\n",
    "        #     keyword_list = keywords.split('、')\n",
    "        # print(keyword_list)\n",
    "        delimiters = '[，,、；;]'  # 包含逗号、中文逗号和顿号\n",
    "        # 使用正则表达式分割字符串\n",
    "        keyword_list = re.split(delimiters, keywords)\n",
    "        # 移除列表中的空字符串（如果有的话）\n",
    "        keyword_list = [keyword.strip() for keyword in keyword_list if keyword.strip()]\n",
    "\n",
    "        return gr.update(choices=keyword_list, value=True, interactive=False)\n",
    "    else:\n",
    "        return gr.update(choices=[\"Error\"], value=None, interactive=False)\n",
    "\n",
    "def sort_results(text):\n",
    "    pattern = r\"正在分析第(\\d+)个网页：\\n网址：\\s*(.*?)向量评分：(?P<score>\\d+(\\.\\d{1,2})?)/100\"\n",
    "\n",
    "    # 使用 findall 查找所有匹配项\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # 提取网页编号、内容和对应的综合评分\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        webpage_number = match[0]\n",
    "        webpage_content = match[1].strip()\n",
    "        score = float(match[2])\n",
    "        results.append((webpage_number, webpage_content, score))\n",
    "\n",
    "    # 按照综合评分排序，分数高的排在前面\n",
    "    sorted_results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # 输出排序后的结果\n",
    "    final_results = \"\"\n",
    "    for result in sorted_results:\n",
    "        final_results += f\"第{result[0]}个网页：\\n{result[1]}\\n\\n向量评分：{result[2]}/100（最高余弦相似度）\\n----------\\n\\n\"\n",
    "    return final_results\n",
    "\n",
    "# 问答函数\n",
    "def chatbot_response(message, chat_history, webpage_text):\n",
    "    # 添加用户的消息到历史记录\n",
    "    chat_history.append((\"用户\", message))\n",
    "    accumulated_text = \"\"\n",
    "    yield chat_history, accumulated_text\n",
    "\n",
    "    llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", stop_sequences=[\"### Response\", \"<|endoftext|>\", \"###\", \"---\"])\n",
    "    embedding = embeddings.OpenAIEmbeddings(check_embedding_ctx_length=False, base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "    text_splitter2 = CharacterTextSplitter(separator=\"\\n\\n\")\n",
    "    texts = text_splitter.split_text(webpage_text)\n",
    "    final_texts = []\n",
    "    for text in texts:\n",
    "        final_texts.extend(text_splitter2.split_text(text))\n",
    "    # 向量化\n",
    "    vectorstore = FAISS.from_texts(final_texts, embedding)\n",
    "\n",
    "    template =\"\"\"{context}\n",
    "1. 仅根据以上给出的信息回答问题，不要使用自身的知识。\n",
    "2. 指明回答参考的文本块号。\n",
    "问题: {question}\n",
    "\n",
    "回答示例（问题：小家电的种类）：\n",
    "回答：按照小家电的使用功能，可以将其分为四类：是厨房小家电产品、家居小家电产品、个人生活小家电产品、个人使用数码产品\n",
    "来源：第2个文本块、第5个文本块、第11个文本块\n",
    "\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    query = message\n",
    "    docs = vectorstore.similarity_search(query)\n",
    "    print(len(docs),\"个doc\")\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(f\"第{index}个文本块：\\n\"+doc.page_content for index, doc in enumerate(docs, start=1))\n",
    "\n",
    "    formated_docs = format_docs(docs)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        # {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = \"\"\n",
    "    for message in retrieval_chain.stream({\"context\": formated_docs, \"question\": query}):\n",
    "        accumulated_text += message\n",
    "        answer += message\n",
    "        yield chat_history, accumulated_text\n",
    "    \n",
    "    accumulated_text += \"\\n\"\n",
    "\n",
    "    pattern = r'第(\\d+)个文本块'\n",
    "    matches = re.findall(pattern, answer)\n",
    "    unique_list = list(set(matches)) # 去重\n",
    "    print(unique_list,\"个unique_list\")\n",
    "    for match in unique_list:\n",
    "        # accumulated_text += (f'第{match}个文本块：\\n'+docs[int(match)-1].page_content+\"\\n\") # 文本块具体内容\n",
    "        accumulated_text += (f'第{match}个文本块')\n",
    "        # 查找网页\n",
    "        article = webpage_text\n",
    "        target_sentence = docs[int(match)-1].page_content\n",
    "\n",
    "        article = article.replace('\\n', '').strip()\n",
    "        target_sentence = target_sentence.replace('\\n', '').strip()\n",
    "        article = re.sub(r'\\s+', ' ', article)\n",
    "        target_sentence = re.sub(r'\\s+', ' ', target_sentence)\n",
    "        \n",
    "        header_pattern = r'第\\d+个网页'\n",
    "        paragraph_pattern = re.compile(rf'({header_pattern})(.*?)(?={header_pattern}|$)', re.DOTALL)\n",
    "        paragraphs = paragraph_pattern.findall(article)\n",
    "        for header, content in paragraphs:\n",
    "            if target_sentence in content:\n",
    "                accumulated_text += f\"来源于：{header}\\n\"\n",
    "                break\n",
    "        else:\n",
    "            accumulated_text += \"来源于：未知\\n\"\n",
    "\n",
    "        yield chat_history, accumulated_text\n",
    "    \n",
    "    # 将模型的回复添加到历史记录\n",
    "    chat_history.append((\"回答\", accumulated_text))\n",
    "    \n",
    "    yield chat_history, \"\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    title = gr.Markdown(\"## 行业信息获取器\")\n",
    "    description = gr.Markdown(\"输入行业，生成相关关键词。\")\n",
    "\n",
    "    with gr.Row():\n",
    "        industry = gr.Textbox(label=\"输入行业名\", lines=1, max_lines=1)\n",
    "        keywords_generated = gr.Textbox(label=\"行业关键词\", lines=8, max_lines=8, show_copy_button=True)\n",
    "        keywords_processed = gr.Textbox(label=\"处理后的关键词\", lines=8, max_lines=8, show_copy_button=True)\n",
    "        \n",
    "\n",
    "    with gr.Row():\n",
    "    # 定义第一个按钮的行为\n",
    "        generate_button = gr.Button(\"生成关键词\")\n",
    "        generate_button.click(fn=generate_keywords, inputs=industry, outputs=keywords_generated)\n",
    "        process_button = gr.Button(\"处理关键词\")\n",
    "        clear_button = gr.Button(\"清除关键词\")\n",
    "        \n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            keywords_to_search = gr.Radio(label=\"搜索关键词\", choices=[])\n",
    "            \n",
    "        with gr.Column():\n",
    "            pages_needed = gr.Slider(label=\"搜索页数\", minimum=1, maximum=20, step=1, value=1)\n",
    "            links_found = gr.Textbox(label=\"获取到的链接\", lines=10, max_lines=10)\n",
    "            submit_button = gr.Button(\"获取链接\")\n",
    "\n",
    "        # 定义第三个按钮的行为\n",
    "        # option_button = gr.Button(\"获取关键词\")\n",
    "        # option_button.click(fn=, inputs=keywords_generated, outputs=)\n",
    "        process_button.click(fn=process_keywords, inputs=keywords_generated, outputs=[keywords_processed, keywords_to_search])\n",
    "        submit_button.click(fn=selection_to_links, inputs=[industry, keywords_to_search, pages_needed], outputs=links_found)\n",
    "        clear_button.click(fn=clear_all, inputs=[], outputs=[industry, keywords_generated, keywords_processed, keywords_to_search])\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            min_slider = gr.Slider(label=\"获取页面最小值\", minimum=0, maximum=20, step=1, value=1)\n",
    "            max_slider = gr.Slider(label=\"获取页面最大值\", minimum=0, maximum=20, step=1, value=0)\n",
    "            no_slider = gr.Textbox(label=\"手动输入（例：1-10，例：25）\", lines=1, max_lines=1)\n",
    "            with gr.Row():\n",
    "                webpage_to_analyze = gr.Textbox(label=\"输入需要分析的页面号\", lines=1, max_lines=1, value=1)\n",
    "                analyze_all = gr.Checkbox(label=\"选择全部\", value=False)\n",
    "            clear_slider = gr.ClearButton(components=[max_slider, no_slider], value=\"重新输入\", )\n",
    "            \n",
    "            \n",
    "        with gr.Column():\n",
    "            webpage_text = gr.Textbox(label=\"网页内容\", lines=16, max_lines=16, show_copy_button=True)\n",
    "            \n",
    "            \n",
    "    with gr.Row():\n",
    "        webpage_button = gr.Button(\"获取网页内容\")\n",
    "        webpage_analyze = gr.Button(\"分析网页内容\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # webpage_to_analyze = gr.Textbox(label=\"输入需要分析的页面号\", lines=1, max_lines=1, value=1)\n",
    "        # analyzed_keywords = gr.CheckboxGroup(label=\"分析关键词\", choices=[])\n",
    "        with gr.Column():\n",
    "            analyzed_text = gr.Textbox(label=\"分析结果\", lines=20, max_lines=20, show_copy_button=True)\n",
    "            sort_button = gr.Button(\"排序\")\n",
    "\n",
    "        sorted_results = gr.Textbox(label=\"排序结果\", lines=20, max_lines=20, show_copy_button=True)\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"问答机器人\")\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(label=\"网页问答\", placeholder=\"输入你的问题\")\n",
    "            clear = gr.Button(\"清除问答记录\")\n",
    "\n",
    "        msg.submit(chatbot_response, [msg, chatbot, webpage_text], [chatbot, msg])\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    minnum = gr.Number(label=\"最小值\", visible=False, interactive=False)\n",
    "    maxnum = gr.Number(label=\"最大值\", visible=False, interactive=False)\n",
    "    webpage_button.click(fn=webpage_to_text, inputs=[links_found, min_slider, max_slider, no_slider], outputs=[webpage_text, minnum, maxnum])\n",
    "    # webpage_analyze.click(fn=analyze_webpage, inputs=[webpage_text, keywords_processed, webpage_to_analyze, industry, keywords_to_search], outputs=[analyzed_text, analyzed_keywords])\n",
    "    webpage_analyze.click(fn=batch_analyze_webpage, inputs=[analyze_all, webpage_text, keywords_processed, webpage_to_analyze, industry, keywords_to_search, minnum, maxnum], outputs=analyzed_text)\n",
    "    sort_button.click(fn=sort_results, inputs=[analyzed_text], outputs=sorted_results)\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
